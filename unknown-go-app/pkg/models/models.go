// Package models provides data structures used throughout the application.
package models

import "time"

// User represents a user in the system
type User struct {
	ID    int    `json:"id"`
	Name  string `json:"name"`
	Email string `json:"email"`
}

// Product represents a product in the system
type Product struct {
	ID    int     `json:"id"`
	Name  string  `json:"name"`
	Price float64 `json:"price"`
}

// LanguageModelProvider represents different LLM providers supported by the application.
// The application can route requests to different AI providers based on this value.
type LanguageModelProvider string

const (
	// ProviderOpenAI represents the OpenAI API provider (for GPT models)
	ProviderOpenAI LanguageModelProvider = "openai"
	// ProviderCopilot represents the GitHub Copilot Chat API provider
	ProviderCopilot LanguageModelProvider = "copilot"
	// ProviderAnthropic represents the Anthropic Claude API provider
	ProviderAnthropic LanguageModelProvider = "anthropic"
	// ProviderGoogle represents the Google AI API provider
	ProviderGoogle LanguageModelProvider = "google"
)

// LanguageModel contains metadata about an LLM including its capabilities,
// rate limits, and provider information. These settings determine how
// requests are routed and rate limited.
type LanguageModel struct {
	// ID is the unique identifier for the model
	ID string `json:"id"`
	// Name is the display name for the model
	Name string `json:"name"`
	// Provider indicates which API provider handles this model
	Provider LanguageModelProvider `json:"provider"`
	// MaxRequestsPerMinute is the maximum number of requests allowed per minute
	MaxRequestsPerMinute int `json:"max_requests_per_minute"`
	// MaxTokensPerMinute is the maximum total tokens allowed per minute
	MaxTokensPerMinute int `json:"max_tokens_per_minute"`
	// MaxInputTokensPerMinute is the maximum input tokens allowed per minute
	MaxInputTokensPerMinute int `json:"max_input_tokens_per_minute"`
	// MaxOutputTokensPerMinute is the maximum output tokens allowed per minute
	MaxOutputTokensPerMinute int `json:"max_output_tokens_per_minute"`
	// MaxTokensPerDay is the maximum total tokens allowed per day
	MaxTokensPerDay int `json:"max_tokens_per_day"`
	// Enabled indicates if the model is currently available for use
	Enabled bool `json:"enabled"`
}

// ModelUsage tracks usage metrics for a model to enforce rate limits.
// This includes usage counters for different time periods and token types.
type ModelUsage struct {
	// UserID is the ID of the user the usage belongs to
	UserID uint64 `json:"user_id"`
	// Provider indicates which LLM provider this usage is for
	Provider LanguageModelProvider `json:"provider"`
	// Model is the name of the model being used
	Model string `json:"model"`
	// RequestsThisMinute counts API requests made in the current minute
	RequestsThisMinute int `json:"requests_this_minute"`
	// TokensThisMinute counts total tokens consumed in the current minute
	TokensThisMinute int `json:"tokens_this_minute"`
	// InputTokensThisMinute counts input tokens in the current minute
	InputTokensThisMinute int `json:"input_tokens_this_minute"`
	// OutputTokensThisMinute counts output tokens in the current minute
	OutputTokensThisMinute int `json:"output_tokens_this_minute"`
	// TokensThisDay counts total tokens consumed in the current day
	TokensThisDay int `json:"tokens_this_day"`
}

// TokenUsage contains details about token usage for billing and rate limiting.
// It breaks down token usage into different categories for fine-grained tracking.
type TokenUsage struct {
	// Input counts tokens in the user's input
	Input int `json:"input"`
	// Output counts tokens generated by the LLM
	Output int `json:"output"`
	// InputCacheRead counts tokens read from cache instead of sending to LLM
	InputCacheRead int `json:"input_cache_read"`
	// InputCacheCreate counts tokens stored in cache for future use
	InputCacheCreate int `json:"input_cache_creation"`
}

// ActiveUserCount contains information about active users for resource allocation.
// This helps scale rate limits based on current system load.
type ActiveUserCount struct {
	// UsersInRecentMinutes is the number of unique users in the past few minutes
	UsersInRecentMinutes int `json:"users_in_recent_minutes"`
	// UsersInRecentDays is the number of unique users in the past few days
	UsersInRecentDays int `json:"users_in_recent_days"`
}

// LLMToken holds claims related to LLM authentication and authorization.
// This represents the decoded content of a JWT used for API authentication.
type LLMToken struct {
	// Iat is the issued at timestamp
	Iat int64 `json:"iat"`
	// Exp is the expiration timestamp
	Exp int64 `json:"exp"`
	// Jti is the JWT ID (unique identifier for the token)
	Jti string `json:"jti"`
	// UserID is the ID of the user the token belongs to
	UserID uint64 `json:"user_id"`
	// MetricsID is used for analytics tracking
	MetricsID string `json:"metrics_id"`
	// GithubUserLogin is the GitHub username of the user
	GithubUserLogin string `json:"github_user_login"`
	// AccountCreatedAt is when the user's account was created
	AccountCreatedAt time.Time `json:"account_created_at"`
	// IsStaff indicates if the user has staff privileges
	IsStaff bool `json:"is_staff"`
	// HasLLMSubscription indicates if the user has a paid subscription
	HasLLMSubscription bool `json:"has_llm_subscription"`
	// MaxMonthlySpendInCents is the maximum allowed spending per month
	MaxMonthlySpendInCents uint32 `json:"max_monthly_spend_in_cents"`
	// CustomMonthlyAllowanceInCents is a custom spending limit (if applicable)
	CustomMonthlyAllowanceInCents *uint32 `json:"custom_llm_monthly_allowance_in_cents,omitempty"`
}
